---
title: 'Tuning performance'
description: 'Different ways to tune the performance'
icon: 'gear'
---

## Choosing the model
We currently only support OpenAI but more is coming soon.

```python
arg = ArgAI(model="openai/gpt-3.5-turbo-0613")
```

## Providing a system prompt
`arg` can accept more than just strings but full conversations. This allows you to add a system prompt which can improve accuracy.
```python
arg([
    {
        "role": "system",
        "content": "..."
    },
    {
        "role": "user",
        "content": "..."
    }
])
```

## Adjust the function definition
Below is a pretty basic function. By default argai will automatically try generating the necessary descriptions for the llm, however you can adjust it to get better performance if you manually describe the function in detail.
```python
def create_file(filename, text):
    with open(filename,'w') as f:
        f.write(text)
        return {"success":True}
```

### Adjust the function description
You can add a description to your function by adding a docstring
```python
def create_file(filename, text):
    """This function will create a file given the content of the file"""
    with open(filename,'w') as f:
        f.write(text)
        return {"success":True}
```

### Adjust the function input parameters
Parameter typing:
```python
def create_file(filename:str, text:str):
    """This function will create a file given the content of the file"""
    with open(filename,'w') as f:
        f.write(text)
        return {"success":True}
```

Parameter descriptions:
```python
```

### Adjust the function output
Make the output of your function more interpretable.
```python
def create_file(filename:str, text:str):
    """This function will create a file given the content of the file"""
    with open(filename,'w') as f:
        f.write(text)
        return "Successfully create the file"
```

You can separate the output for LLM vs actual output. You do this by returning your output in a dictionary with the key `_argai` this will actually make sure that the llm reads the `_argai` output instead of the actual function output.
```python
def create_file(filename:str, text:str):
    """This function will create a file given the content of the file"""
    with open(filename,'w') as f:
        f.write(text)
        return {
            "_argai" : "Successfully create the file",
            "success": True
        }
```

## Adjusting the definition with pydantic

```python
from argai import ArgAI
from pydantic import BasedModel, Field

arg = ArgAI()

class MyFunction(BasedModel):
    """..."""
    input: str = Field(..., description="...")
    another_input: str = Field(..., description="...")

@arg.register(base_model=MyFunction)
def my_function(input, another_input):
    return ...
```

## Adjusting via jsonschema/openai jsonschema

If you input a jsonschema it will automatically input into the appropriate openai jsonschema
```python
from argai import ArgAI
from pydantic import BasedModel, Field

arg = ArgAI()

my_func_schema = {
    ...
}

@arg.register(jsonschema=my_func_schema)
def my_function(input, another_input):
    return ...
```
